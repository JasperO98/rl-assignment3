{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jasper/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jasper/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jasper/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jasper/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jasper/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jasper/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jasper/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jasper/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jasper/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jasper/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jasper/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jasper/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import regularizers\n",
    "from tensorflow.keras import backend as kb\n",
    "from keras import optimizers\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.57776089,  0.        ])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing a random game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_a_random_game_first():\n",
    "    for step_index in range(goal_steps):\n",
    "        #env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        print(\"Step {}:\".format(step_index))\n",
    "        print(\"action: {}\".format(action))\n",
    "        print(\"observation: {}\".format(observation))\n",
    "        print(\"reward: {}\".format(reward))\n",
    "        print(\"done: {}\".format(done))\n",
    "        print(\"info: {}\".format(info))\n",
    "        if done:\n",
    "            break\n",
    "    env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0:\n",
      "action: 1\n",
      "observation: [-4.81833566e-01 -3.14762199e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 1:\n",
      "action: 0\n",
      "observation: [-0.48346075 -0.00162718]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 2:\n",
      "action: 1\n",
      "observation: [-0.48538824 -0.00192749]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 3:\n",
      "action: 2\n",
      "observation: [-0.48660168 -0.00121344]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 4:\n",
      "action: 2\n",
      "observation: [-0.48709203 -0.00049035]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 5:\n",
      "action: 1\n",
      "observation: [-0.48785564 -0.0007636 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 6:\n",
      "action: 0\n",
      "observation: [-0.4898868  -0.00203116]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 7:\n",
      "action: 0\n",
      "observation: [-0.49317038 -0.00328357]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 8:\n",
      "action: 2\n",
      "observation: [-0.49568185 -0.00251147]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 9:\n",
      "action: 0\n",
      "observation: [-0.49940245 -0.0037206 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 10:\n",
      "action: 1\n",
      "observation: [-0.50330436 -0.00390192]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 11:\n",
      "action: 1\n",
      "observation: [-0.50735839 -0.00405403]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 12:\n",
      "action: 0\n",
      "observation: [-0.51253418 -0.00517578]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 13:\n",
      "action: 2\n",
      "observation: [-0.51679293 -0.00425875]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 14:\n",
      "action: 1\n",
      "observation: [-0.52110272 -0.00430979]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 15:\n",
      "action: 2\n",
      "observation: [-0.52443124 -0.00332851]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 16:\n",
      "action: 1\n",
      "observation: [-0.52775351 -0.00332227]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 17:\n",
      "action: 0\n",
      "observation: [-0.53204462 -0.00429111]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 18:\n",
      "action: 1\n",
      "observation: [-0.53627239 -0.00422777]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 19:\n",
      "action: 0\n",
      "observation: [-0.54140514 -0.00513274]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 20:\n",
      "action: 0\n",
      "observation: [-0.5474044  -0.00599926]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 21:\n",
      "action: 1\n",
      "observation: [-0.55322527 -0.00582087]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 22:\n",
      "action: 2\n",
      "observation: [-0.55782423 -0.00459896]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 23:\n",
      "action: 2\n",
      "observation: [-0.56116696 -0.00334272]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 24:\n",
      "action: 2\n",
      "observation: [-0.56322851 -0.00206156]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 25:\n",
      "action: 0\n",
      "observation: [-0.56599355 -0.00276503]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 26:\n",
      "action: 1\n",
      "observation: [-0.56844148 -0.00244793]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 27:\n",
      "action: 2\n",
      "observation: [-0.5695541  -0.00111262]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 28:\n",
      "action: 1\n",
      "observation: [-0.57032315 -0.00076905]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 29:\n",
      "action: 0\n",
      "observation: [-0.57174292 -0.00141976]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 30:\n",
      "action: 2\n",
      "observation: [-5.71802853e-01 -5.99363777e-05]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 31:\n",
      "action: 2\n",
      "observation: [-0.57050252  0.00130034]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 32:\n",
      "action: 1\n",
      "observation: [-0.56885156  0.00165095]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 33:\n",
      "action: 2\n",
      "observation: [-0.56586226  0.00298931]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 34:\n",
      "action: 0\n",
      "observation: [-0.56355682  0.00230544]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 35:\n",
      "action: 2\n",
      "observation: [-0.55995242  0.0036044 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 36:\n",
      "action: 2\n",
      "observation: [-0.5550759   0.00487652]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 37:\n",
      "action: 2\n",
      "observation: [-0.54896366  0.00611224]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 38:\n",
      "action: 0\n",
      "observation: [-0.54366136  0.0053023 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 39:\n",
      "action: 0\n",
      "observation: [-0.53920868  0.00445268]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 40:\n",
      "action: 2\n",
      "observation: [-0.53363898  0.00556971]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 41:\n",
      "action: 0\n",
      "observation: [-0.52899398  0.004645  ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 42:\n",
      "action: 0\n",
      "observation: [-0.52530852  0.00368546]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 43:\n",
      "action: 2\n",
      "observation: [-0.52061024  0.00469828]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 44:\n",
      "action: 1\n",
      "observation: [-0.51593437  0.00467587]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 45:\n",
      "action: 0\n",
      "observation: [-0.51231598  0.00361839]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 46:\n",
      "action: 0\n",
      "observation: [-0.5097822   0.00253379]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 47:\n",
      "action: 2\n",
      "observation: [-0.506352    0.00343019]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 48:\n",
      "action: 2\n",
      "observation: [-0.50205111  0.0043009 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 49:\n",
      "action: 2\n",
      "observation: [-0.4969117  0.0051394]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 50:\n",
      "action: 2\n",
      "observation: [-0.49097224  0.00593946]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 51:\n",
      "action: 2\n",
      "observation: [-0.48427708  0.00669516]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 52:\n",
      "action: 2\n",
      "observation: [-0.47687616  0.00740093]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 53:\n",
      "action: 0\n",
      "observation: [-0.47082451  0.00605165]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 54:\n",
      "action: 2\n",
      "observation: [-0.46416701  0.0066575 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 55:\n",
      "action: 0\n",
      "observation: [-0.45895289  0.00521412]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 56:\n",
      "action: 1\n",
      "observation: [-0.45422059  0.00473231]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 57:\n",
      "action: 1\n",
      "observation: [-0.45000487  0.00421572]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 58:\n",
      "action: 2\n",
      "observation: [-0.44533663  0.00466824]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 59:\n",
      "action: 2\n",
      "observation: [-0.44024998  0.00508665]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 60:\n",
      "action: 2\n",
      "observation: [-0.43478195  0.00546803]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 61:\n",
      "action: 0\n",
      "observation: [-0.4309722   0.00380975]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 62:\n",
      "action: 0\n",
      "observation: [-0.42884824  0.00212396]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 63:\n",
      "action: 1\n",
      "observation: [-0.42742537  0.00142286]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 64:\n",
      "action: 1\n",
      "observation: [-0.42671384  0.00071153]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 65:\n",
      "action: 2\n",
      "observation: [-0.42571876  0.00099508]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 66:\n",
      "action: 0\n",
      "observation: [-0.42644728 -0.00072852]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 67:\n",
      "action: 2\n",
      "observation: [-0.42689416 -0.00044688]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 68:\n",
      "action: 0\n",
      "observation: [-0.42905619 -0.00216203]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 69:\n",
      "action: 2\n",
      "observation: [-0.43091783 -0.00186163]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 70:\n",
      "action: 2\n",
      "observation: [-0.43246565 -0.00154782]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 71:\n",
      "action: 2\n",
      "observation: [-0.43368848 -0.00122284]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 72:\n",
      "action: 1\n",
      "observation: [-0.4355775  -0.00188902]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 73:\n",
      "action: 2\n",
      "observation: [-0.43711903 -0.00154153]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 74:\n",
      "action: 2\n",
      "observation: [-0.4383019  -0.00118288]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 75:\n",
      "action: 2\n",
      "observation: [-0.43911755 -0.00081564]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 76:\n",
      "action: 2\n",
      "observation: [-0.43956004 -0.00044249]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 77:\n",
      "action: 1\n",
      "observation: [-0.44062617 -0.00106613]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 78:\n",
      "action: 0\n",
      "observation: [-0.44330818 -0.00268201]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 79:\n",
      "action: 2\n",
      "observation: [-0.44558657 -0.00227839]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 80:\n",
      "action: 1\n",
      "observation: [-0.44844472 -0.00285815]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 81:\n",
      "action: 2\n",
      "observation: [-0.45086176 -0.00241704]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 82:\n",
      "action: 2\n",
      "observation: [-0.45282002 -0.00195825]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 83:\n",
      "action: 1\n",
      "observation: [-0.45530513 -0.00248511]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 84:\n",
      "action: 2\n",
      "observation: [-0.45729887 -0.00199374]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 85:\n",
      "action: 0\n",
      "observation: [-0.46078659 -0.00348772]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 86:\n",
      "action: 1\n",
      "observation: [-0.46474261 -0.00395603]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 87:\n",
      "action: 1\n",
      "observation: [-0.46913777 -0.00439516]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 88:\n",
      "action: 0\n",
      "observation: [-0.47493957 -0.0058018 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 89:\n",
      "action: 0\n",
      "observation: [-0.48210502 -0.00716545]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 90:\n",
      "action: 0\n",
      "observation: [-0.49058087 -0.00847585]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 91:\n",
      "action: 0\n",
      "observation: [-0.50030395 -0.00972308]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 92:\n",
      "action: 0\n",
      "observation: [-0.5112016  -0.01089765]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 93:\n",
      "action: 0\n",
      "observation: [-0.5231922  -0.01199061]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 94:\n",
      "action: 0\n",
      "observation: [-0.53618586 -0.01299365]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 95:\n",
      "action: 2\n",
      "observation: [-0.54808513 -0.01189927]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 96:\n",
      "action: 0\n",
      "observation: [-0.56080092 -0.01271579]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 97:\n",
      "action: 2\n",
      "observation: [-0.57223827 -0.01143735]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 98:\n",
      "action: 0\n",
      "observation: [-0.58431213 -0.01207385]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 99:\n",
      "action: 0\n",
      "observation: [-0.59693314 -0.01262101]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 100:\n",
      "action: 2\n",
      "observation: [-0.60800857 -0.01107543]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 101:\n",
      "action: 2\n",
      "observation: [-0.61745768 -0.0094491 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 102:\n",
      "action: 0\n",
      "observation: [-0.6272121  -0.00975443]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 103:\n",
      "action: 0\n",
      "observation: [-0.63720188 -0.00998978]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 104:\n",
      "action: 0\n",
      "observation: [-0.64735604 -0.01015416]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 105:\n",
      "action: 1\n",
      "observation: [-0.65660319 -0.00924715]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 106:\n",
      "action: 0\n",
      "observation: [-0.66587907 -0.00927588]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 107:\n",
      "action: 0\n",
      "observation: [-0.67511996 -0.00924089]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 108:\n",
      "action: 0\n",
      "observation: [-0.68426317 -0.00914321]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 109:\n",
      "action: 2\n",
      "observation: [-0.69124752 -0.00698435]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 110:\n",
      "action: 1\n",
      "observation: [-0.69702684 -0.00577932]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 111:\n",
      "action: 0\n",
      "observation: [-0.70256335 -0.00553651]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 112:\n",
      "action: 2\n",
      "observation: [-0.70582118 -0.00325783]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 113:\n",
      "action: 0\n",
      "observation: [-0.70877941 -0.00295823]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 114:\n",
      "action: 1\n",
      "observation: [-0.71041913 -0.00163972]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 115:\n",
      "action: 2\n",
      "observation: [-7.09729902e-01  6.89225196e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 116:\n",
      "action: 0\n",
      "observation: [-0.70871612  0.00101379]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 117:\n",
      "action: 2\n",
      "observation: [-0.70538422  0.00333189]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 118:\n",
      "action: 0\n",
      "observation: [-0.70175553  0.0036287 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 119:\n",
      "action: 1\n",
      "observation: [-0.69685336  0.00490216]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 120:\n",
      "action: 0\n",
      "observation: [-0.69170951  0.00514385]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 121:\n",
      "action: 2\n",
      "observation: [-0.68435761  0.00735191]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 122:\n",
      "action: 1\n",
      "observation: [-0.67584621  0.0085114 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 123:\n",
      "action: 2\n",
      "observation: [-0.66523224  0.01061397]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 124:\n",
      "action: 1\n",
      "observation: [-0.6535877   0.01164454]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 125:\n",
      "action: 1\n",
      "observation: [-0.64099276  0.01259494]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 126:\n",
      "action: 2\n",
      "observation: [-0.62653546  0.0144573 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 127:\n",
      "action: 0\n",
      "observation: [-0.61231834  0.01421712]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 128:\n",
      "action: 1\n",
      "observation: [-0.59744366  0.01487468]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 129:\n",
      "action: 1\n",
      "observation: [-0.58201966  0.015424  ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 130:\n",
      "action: 1\n",
      "observation: [-0.56615974  0.01585992]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 131:\n",
      "action: 1\n",
      "observation: [-0.54998148  0.01617826]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 132:\n",
      "action: 2\n",
      "observation: [-0.53260556  0.01737592]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 133:\n",
      "action: 1\n",
      "observation: [-0.51516209  0.01744347]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 134:\n",
      "action: 0\n",
      "observation: [-0.49878189  0.0163802 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 135:\n",
      "action: 2\n",
      "observation: [-0.48158765  0.01719424]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 136:\n",
      "action: 0\n",
      "observation: [-0.46570766  0.01587999]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 137:\n",
      "action: 1\n",
      "observation: [-0.45025967  0.01544799]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 138:\n",
      "action: 2\n",
      "observation: [-0.4343573   0.01590237]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 139:\n",
      "action: 0\n",
      "observation: [-0.42011627  0.01424103]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 140:\n",
      "action: 1\n",
      "observation: [-0.40663895  0.01347732]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 141:\n",
      "action: 2\n",
      "observation: [-0.39302095  0.01361801]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 142:\n",
      "action: 0\n",
      "observation: [-0.38135742  0.01166353]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 143:\n",
      "action: 1\n",
      "observation: [-0.37072862  0.0106288 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 144:\n",
      "action: 1\n",
      "observation: [-0.36120657  0.00952205]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 145:\n",
      "action: 0\n",
      "observation: [-0.35385486  0.00735171]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 146:\n",
      "action: 2\n",
      "observation: [-0.34672191  0.00713295]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 147:\n",
      "action: 1\n",
      "observation: [-0.34085416  0.00586775]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 148:\n",
      "action: 0\n",
      "observation: [-0.33728936  0.0035648 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 149:\n",
      "action: 2\n",
      "observation: [-0.33405025  0.00323911]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 150:\n",
      "action: 1\n",
      "observation: [-0.33215737  0.00189288]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 151:\n",
      "action: 0\n",
      "observation: [-0.33262266 -0.00046529]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 152:\n",
      "action: 2\n",
      "observation: [-0.33344319 -0.00082053]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 153:\n",
      "action: 2\n",
      "observation: [-0.33461378 -0.00117059]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 154:\n",
      "action: 2\n",
      "observation: [-0.33612704 -0.00151326]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 155:\n",
      "action: 2\n",
      "observation: [-0.33797337 -0.00184633]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 156:\n",
      "action: 1\n",
      "observation: [-0.34114104 -0.00316768]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 157:\n",
      "action: 1\n",
      "observation: [-0.34560983 -0.00446879]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 158:\n",
      "action: 2\n",
      "observation: [-0.350351   -0.00474117]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 159:\n",
      "action: 0\n",
      "observation: [-0.35733382 -0.00698281]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 160:\n",
      "action: 0\n",
      "observation: [-0.36651255 -0.00917873]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 161:\n",
      "action: 2\n",
      "observation: [-0.3758263  -0.00931375]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 162:\n",
      "action: 1\n",
      "observation: [-0.3862124 -0.0103861]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 163:\n",
      "action: 2\n",
      "observation: [-0.39659997 -0.01038757]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 164:\n",
      "action: 0\n",
      "observation: [-0.40891716 -0.01231718]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 165:\n",
      "action: 0\n",
      "observation: [-0.42307759 -0.01416043]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 166:\n",
      "action: 1\n",
      "observation: [-0.43798055 -0.01490296]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 167:\n",
      "action: 2\n",
      "observation: [-0.45251861 -0.01453806]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 168:\n",
      "action: 0\n",
      "observation: [-0.46858574 -0.01606713]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 169:\n",
      "action: 1\n",
      "observation: [-0.4850636  -0.01647786]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 170:\n",
      "action: 1\n",
      "observation: [-0.50182983 -0.01676623]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 171:\n",
      "action: 2\n",
      "observation: [-0.51775921 -0.01592938]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 172:\n",
      "action: 1\n",
      "observation: [-0.53373239 -0.01597318]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 173:\n",
      "action: 0\n",
      "observation: [-0.55062957 -0.01689718]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 174:\n",
      "action: 0\n",
      "observation: [-0.56832425 -0.01769468]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 175:\n",
      "action: 2\n",
      "observation: [-0.58468449 -0.01636024]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 176:\n",
      "action: 2\n",
      "observation: [-0.59958915 -0.01490466]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 177:\n",
      "action: 2\n",
      "observation: [-0.6129288  -0.01333965]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 178:\n",
      "action: 0\n",
      "observation: [-0.62660647 -0.01367767]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 179:\n",
      "action: 2\n",
      "observation: [-0.63852382 -0.01191735]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 180:\n",
      "action: 0\n",
      "observation: [-0.65059621 -0.01207239]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 181:\n",
      "action: 0\n",
      "observation: [-0.66273899 -0.01214278]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 182:\n",
      "action: 1\n",
      "observation: [-0.67386826 -0.01112927]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 183:\n",
      "action: 2\n",
      "observation: [-0.6829083  -0.00904004]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 184:\n",
      "action: 2\n",
      "observation: [-0.68979849 -0.00689019]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 185:\n",
      "action: 1\n",
      "observation: [-0.69549318 -0.0056947 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 186:\n",
      "action: 2\n",
      "observation: [-0.69895506 -0.00346187]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 187:\n",
      "action: 2\n",
      "observation: [-0.70016159 -0.00120653]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 188:\n",
      "action: 0\n",
      "observation: [-0.70110496 -0.00094337]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 189:\n",
      "action: 0\n",
      "observation: [-7.01779063e-01 -6.74106783e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 190:\n",
      "action: 1\n",
      "observation: [-7.01179554e-01  5.99508225e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 191:\n",
      "action: 1\n",
      "observation: [-0.6993103   0.00186925]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 192:\n",
      "action: 2\n",
      "observation: [-0.6951834  0.0041269]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 193:\n",
      "action: 1\n",
      "observation: [-0.6898257  0.0053577]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 194:\n",
      "action: 0\n",
      "observation: [-0.68427233  0.00555337]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 195:\n",
      "action: 0\n",
      "observation: [-0.67856004  0.0057123 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 196:\n",
      "action: 0\n",
      "observation: [-0.67272694  0.00583309]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 197:\n",
      "action: 1\n",
      "observation: [-0.66581233  0.00691462]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 198:\n",
      "action: 2\n",
      "observation: [-0.65686317  0.00894915]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 199:\n",
      "action: 2\n",
      "observation: [-0.64594095  0.01092222]\n",
      "reward: -1.0\n",
      "done: True\n",
      "info: {'TimeLimit.truncated': True}\n"
     ]
    }
   ],
   "source": [
    "play_a_random_game_first()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run with GPU/CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0', '/device:XLA_GPU:0', '/device:XLA_CPU:0']\n"
     ]
    }
   ],
   "source": [
    "def run(device, function, repeats, **kwargs):\n",
    "    \"\"\"\n",
    "    Run a given function on the specified device with the provided keyword arguments\n",
    "    \"\"\"\n",
    "    with tf.device(device):\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Run function with all additional keyword arguments provided\n",
    "        model = function(**kwargs)\n",
    "\n",
    "        t = time.time() - t0\n",
    "    return model\n",
    "\n",
    "\n",
    "# Might be different on other pc\n",
    "cpu = '/device:CPU:0'\n",
    "gpu = '/device:GPU:0'\n",
    "\n",
    "local_device_protos = device_lib.list_local_devices()\n",
    "print([x.name for x in local_device_protos])\n",
    "#tf.device('/device:GPU:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_size, output_size):\n",
    "    ## Building the nnet that approximates q \n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim = input_size , activation = 'relu'))\n",
    "    model.add(Dense(32, activation = 'relu'))\n",
    "    model.add(Dense(output_size, activation = 'linear'))\n",
    "    model.compile(optimizer=optimizers.Adam(), loss = 'mse')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.shape[0])\n",
    "print(env.action_space.n)\n",
    "model = build_model(env.observation_space.shape[0], env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay(replay_model, replay_memory, batch_size=32):\n",
    "    # choose randomly from replay\n",
    "    batch = np.random.choice(replay_memory, batch_size, replace=True)\n",
    "    \n",
    "    # create seperate list for state, action, reward, next action and done\n",
    "    s_l = np.array(list(map(lambda x: x['state'], batch)))\n",
    "    a_l = np.array(list(map(lambda x: x['action'], batch)))\n",
    "    r_l = np.array(list(map(lambda x: x['reward'], batch)))\n",
    "    sprime_l = np.array(list(map(lambda x: x['next_state'], batch)))\n",
    "    done_l   = np.array(list(map(lambda x: x['done'], batch)))\n",
    "    \n",
    "    # Find q(s', a') for all possible actions a'. Store in list\n",
    "    # We'll use the maximum of these values for q-update  \n",
    "    qvals_sprime_l = replay_model.predict(sprime_l)\n",
    "    \n",
    "    # Find q(s,a) for all possible actions a. Store in list\n",
    "    target_f = replay_model.predict(s_l)\n",
    "    \n",
    "    # q-update target\n",
    "    # For the action we took, use the q-update value  \n",
    "    # For other actions, use the current nnet predicted value\n",
    "    for i,(s,a,r,qvals_sprime, done) in enumerate(zip(s_l,a_l,r_l,qvals_sprime_l, done_l)): \n",
    "        if not done:\n",
    "            target = r + gamma * np.max(qvals_sprime)\n",
    "        else:\n",
    "            target = r\n",
    "        target_f[i][a] = target\n",
    "    \n",
    "    # Update weights of neural network with fit() \n",
    "    # Loss function is 0 for actions we didn't take\n",
    "    replay_model.fit(s_l, target_f, epochs=1, verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model=model, episodes=1000, gamma=0.99, epsilon=1, batch_size=32, mem_max_size=100000,\n",
    "                copy=49):     \n",
    "    reward_sums = [] # store score of each episode\n",
    "    replay_memory = [] # replay memory\n",
    "    \n",
    "    replay_model = model\n",
    "    \n",
    "    for ep in range(episodes): \n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        reward_sum = 0\n",
    "        \n",
    "        # Stop when top is reached\n",
    "        while not done:\n",
    "            # render game to see car learning\n",
    "            # env.render()\n",
    "\n",
    "            # Feedforward pass for current state to get predicted q-values for all actions \n",
    "            qvals_state = model.predict(state.reshape(1,-1))\n",
    "            \n",
    "            # Choose action which is epsilon greedy\n",
    "            if np.random.random() < epsilon:\n",
    "                # If random float < epsilon take random action\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Take the best predicted action (index of best action)\n",
    "                action = np.argmax(qvals_state)\n",
    "            \n",
    "            # Take step, store results \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            reward_sum += reward\n",
    "            \n",
    "            # add to memory, respecting memory buffer limit \n",
    "            if len(replay_memory) > mem_max_size:\n",
    "                replay_memory.pop(0)\n",
    "            replay_memory.append({\"state\":state,\"action\":action,\"reward\":reward,\n",
    "                                  \"next_state\":next_state,\"done\":done})\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "            \n",
    "            # Train the nnet that approximates q(s,a), using the replay memory\n",
    "            replay_model = replay(replay_model, replay_memory, batch_size = batch_size)\n",
    "            \n",
    "            # Decrease epsilon until we hit a target threshold \n",
    "            if epsilon > 0.01:\n",
    "                epsilon -= 0.001\n",
    "        \n",
    "        if ep != 0 and not ep % copy:\n",
    "            model = replay_model\n",
    "            \n",
    "        if not ep % int(episodes*0.1):\n",
    "            print(\"Iteration: {}, Total reward: {}\".format(ep, reward_sum))\n",
    "        reward_sums.append(reward_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Total reward: -200.0\n",
      "Iteration: 100, Total reward: -106.0\n",
      "Iteration: 200, Total reward: -116.0\n",
      "Iteration: 300, Total reward: -106.0\n",
      "Iteration: 400, Total reward: -122.0\n",
      "Iteration: 500, Total reward: -164.0\n"
     ]
    }
   ],
   "source": [
    "trained_model = run(device=gpu, function=train_model, repeats=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "choices = []\n",
    "for each_game in range(100):\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    for step_index in range(goal_steps):\n",
    "        # Uncomment this line if you want to see how our bot playing\n",
    "        # env.render()\n",
    "        \n",
    "        action = np.argmax(trained_model.predict(state))\n",
    "        \n",
    "        state, reward, done, info = env.step(action)\n",
    "        \n",
    "        score += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    scores.append(score)\n",
    "\n",
    "print(scores)\n",
    "print('Average Score:',sum(scores)/len(scores))\n",
    "print('choice 1:{}  choice 0:{} choice 2:{}'.format(choices.count(1)/len(choices),choices.count(0)/len(choices),choices.count(2)/len(choices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
